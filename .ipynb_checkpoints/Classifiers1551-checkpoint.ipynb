{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import tokenize_uk\n",
    "from langdetect import detect\n",
    "import json\n",
    "import os.path\n",
    "import pickle\n",
    "import numpy as np\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERMEDIATE_RESULTS_PATH = \"cleared_texts.csv\"\n",
    "INTERMEDIATE_RESULTS = os.path.isfile(INTERMEDIATE_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('ubercorpus.cased.tokenized.word2vec.300d', binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = '1551'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "with open('ukrainian-stopwords.txt','r') as f:\n",
    "    for line in f:\n",
    "        for word in line.split():\n",
    "            stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_raw_data():\n",
    "    \n",
    "    texts = []\n",
    "\n",
    "    for fname in tqdm(sorted(os.listdir(TEXT_DATA_DIR))):\n",
    "        fpath = os.path.join(TEXT_DATA_DIR, fname)\n",
    "        f = open(fpath, encoding='utf-8')\n",
    "        t = f.read()\n",
    "        res = [x.split(\"\\n\", 1) for x in t.split(\"\\n\\n\\n\")]\n",
    "        for z in res:\n",
    "            if (len(z) == 2):\n",
    "                try:\n",
    "                    if (detect(z[1]) != 'uk'):\n",
    "                        # Filter out non-ukrainian texts.\n",
    "                        continue\n",
    "                    text = tokenize_uk.tokenize_text(z[1])\n",
    "                    text = [item for sublist in text for item in sublist]\n",
    "                    text = [item for sublist in text for item in sublist]\n",
    "                    text = list(filter(lambda x: len(x) > 2 and x not in stopwords and x in model.vocab, text))\n",
    "                    # flattening needs to be done twice because text may have multiple paragraphs\n",
    "                    texts.append({\"key\": z[0], \"text\": text, \"label\": fname})\n",
    "                except:\n",
    "                    continue\n",
    "        f.close()\n",
    "\n",
    "    print('Found %s texts.' % len(texts))\n",
    "\n",
    "    res = pd.DataFrame(texts)\n",
    "    res.to_csv(INTERMEDIATE_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not INTERMEDIATE_RESULTS:\n",
    "    parse_raw_data()\n",
    "\n",
    "texts = pd.read_csv(\"cleared_texts.csv\")\n",
    "texts = texts[texts[\"text\"] != \"[]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62425it [01:00, 1027.81it/s]\n"
     ]
    }
   ],
   "source": [
    "summarized_vectors = {}\n",
    "failed_keys = []\n",
    "for pos, row in tqdm(texts.iterrows()):\n",
    "    try:\n",
    "        words = []\n",
    "        strfromcsv = \"words = \" + row[\"text\"]\n",
    "        exec(strfromcsv)\n",
    "        summarized_vectors[row[\"key\"]] = np.sum(model[words], axis=0)\n",
    "    except:\n",
    "        failed_keys.append(row[\"key\"])\n",
    "\n",
    "texts = texts[~texts.text.isin(failed_keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_Classifer():\n",
    "    \n",
    "    def __init__(self, x_train):\n",
    "        self.x_train = x_train\n",
    "    \n",
    "    def fit(self, verbose = 0):\n",
    "        x_train = self.x_train\n",
    "        X = [summarized_vectors.get(key) for key in x_train['key']]\n",
    "        y = x_train['label']\n",
    "        classifier = KNeighborsClassifier(n_neighbors=5, metric='cosine')\n",
    "        classifier.fit(X, y) \n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(texts, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class instance initialized.\n"
     ]
    }
   ],
   "source": [
    "knn = KNN_Classifer(X_train)\n",
    "classifier = knn.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test, c):\n",
    "        y_predict = []\n",
    "        \n",
    "        for i in tqdm(range(x_test.shape[0])):\n",
    "            lbl = c.predict([summarized_vectors[X_test.iloc[i]['key']]])[0]\n",
    "            y_predict.append(lbl)\n",
    "        \n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/18728 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/18728 [00:00<4:59:16,  1.04it/s]\u001b[A\n",
      "  0%|          | 2/18728 [00:01<4:14:08,  1.23it/s]\u001b[A\n",
      "  0%|          | 3/18728 [00:01<3:20:37,  1.56it/s]\u001b[A\n",
      "  0%|          | 4/18728 [00:01<2:39:00,  1.96it/s]\u001b[A\n",
      "  0%|          | 5/18728 [00:02<2:09:32,  2.41it/s]\u001b[A\n",
      "  0%|          | 6/18728 [00:02<1:48:44,  2.87it/s]\u001b[A\n",
      "  0%|          | 7/18728 [00:02<1:36:58,  3.22it/s]\u001b[A\n",
      "  0%|          | 8/18728 [00:02<1:23:28,  3.74it/s]\u001b[A\n",
      "  0%|          | 9/18728 [00:02<1:12:58,  4.27it/s]\u001b[A\n",
      "  0%|          | 10/18728 [00:02<1:07:40,  4.61it/s]\u001b[A\n",
      "  0%|          | 11/18728 [00:03<1:01:19,  5.09it/s]\u001b[A\n",
      "  0%|          | 12/18728 [00:03<56:37,  5.51it/s]  \u001b[A\n",
      "  0%|          | 13/18728 [00:03<54:32,  5.72it/s]\u001b[A\n",
      "  0%|          | 14/18728 [00:03<54:19,  5.74it/s]\u001b[A\n",
      "  0%|          | 15/18728 [00:03<52:50,  5.90it/s]\u001b[A\n",
      "  0%|          | 16/18728 [00:03<54:15,  5.75it/s]\u001b[A\n",
      "  0%|          | 17/18728 [00:04<59:12,  5.27it/s]\u001b[A\n",
      "  0%|          | 18/18728 [00:04<1:06:19,  4.70it/s]\u001b[A\n",
      "  0%|          | 19/18728 [00:04<1:01:03,  5.11it/s]\u001b[A\n",
      "  0%|          | 20/18728 [00:04<59:29,  5.24it/s]  \u001b[A\n",
      "  0%|          | 21/18728 [00:05<1:01:56,  5.03it/s]\u001b[A\n",
      "  0%|          | 22/18728 [00:05<1:04:23,  4.84it/s]\u001b[A\n",
      "  0%|          | 23/18728 [00:05<1:06:55,  4.66it/s]\u001b[A\n",
      "  0%|          | 24/18728 [00:05<1:01:05,  5.10it/s]\u001b[A\n",
      "  0%|          | 25/18728 [00:05<58:26,  5.33it/s]  \u001b[A\n",
      "  0%|          | 26/18728 [00:05<58:07,  5.36it/s]\u001b[A\n",
      "  0%|          | 27/18728 [00:06<56:04,  5.56it/s]\u001b[A\n",
      "  0%|          | 28/18728 [00:06<56:17,  5.54it/s]\u001b[A\n",
      "  0%|          | 29/18728 [00:06<54:31,  5.72it/s]\u001b[A\n",
      "  0%|          | 30/18728 [00:06<52:15,  5.96it/s]\u001b[A\n",
      "  0%|          | 31/18728 [00:06<51:36,  6.04it/s]\u001b[A\n",
      "  0%|          | 32/18728 [00:06<51:51,  6.01it/s]\u001b[A\n",
      "Exception in thread Thread-85:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 102, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 18728/18728 [31:11<00:00, 10.01it/s]\n"
     ]
    }
   ],
   "source": [
    "y = predict(X_test, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_rate(predicted):\n",
    "    error_rate = sum(1 for i, j in zip(predicted, X_test['label']) if i != j) / len(X_test['label'])\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_Classifer():\n",
    "    \n",
    "    def __init__(self, x_train):\n",
    "        self.x_train = x_train\n",
    "    \n",
    "    def fit(self, verbose = 0):\n",
    "        x_train = self.x_train\n",
    "        X = [summarized_vectors.get(key) for key in x_train['key']]\n",
    "        y = x_train['label']\n",
    "        clf = svm.SVC(gamma='scale')\n",
    "        classifier = svm.SVC(gamma='scale')\n",
    "        classifier.fit(X, y)\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM_Classifer(X_train)\n",
    "classifier_svm = svm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18728/18728 [26:17<00:00, 11.88it/s] \n"
     ]
    }
   ],
   "source": [
    "y_svm = predict(X_test, classifier_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM error rate: 0.5866616830414353\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM error rate: \" + str(calculate_error_rate(y_svm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN error rate: 0.5866616830414353\n"
     ]
    }
   ],
   "source": [
    "print(\"KNN error rate: \" + str(calculate_error_rate(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
